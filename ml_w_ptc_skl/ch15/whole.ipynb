{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e62ee9c6",
   "metadata": {},
   "source": [
    "# Ch15: Modeling Sequential Data Using Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c4850",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f1dc6",
   "metadata": {},
   "source": [
    "# Building an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5f9a4",
   "metadata": {},
   "source": [
    "# LSTM for sentiment analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b15f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 28 07:34:35 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe33a173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:15: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipython-input-2179598426.py:15: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
      "/tmp/ipython-input-2179598426.py:17: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Building vocabulary...\n",
      "Vocab size: 69299\n",
      "Train samples: 20000\n",
      "Valid samples: 5000\n",
      "Test samples: 25000\n",
      "\n",
      "Batch shapes:\n",
      "Text batch: torch.Size([32, 575])\n",
      "Label batch: torch.Size([32])\n",
      "Lengths: tensor([172, 394, 400, 160, 540])\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Starting training...\n",
      "Epoch 1 | Train Acc: 0.6402 | Train Loss: 0.6220 | Val Acc: 0.6937 | Val Loss: 0.5503\n",
      "Epoch 2 | Train Acc: 0.7879 | Train Loss: 0.4538 | Val Acc: 0.8193 | Val Loss: 0.4134\n",
      "Epoch 3 | Train Acc: 0.8524 | Train Loss: 0.3408 | Val Acc: 0.8063 | Val Loss: 0.4267\n",
      "Epoch 4 | Train Acc: 0.8910 | Train Loss: 0.2646 | Val Acc: 0.8666 | Val Loss: 0.3218\n",
      "Epoch 5 | Train Acc: 0.9167 | Train Loss: 0.2121 | Val Acc: 0.8885 | Val Loss: 0.2835\n",
      "\n",
      "Evaluating on test set...\n",
      "Test Accuracy: 0.8807 | Test Loss: 0.3145\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER\n",
    "# ============================================================================\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall(\n",
    "        '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()\n",
    "    )\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = text.split()\n",
    "    return tokenized\n",
    "\n",
    "# ============================================================================\n",
    "# VOCABULARY WRAPPER\n",
    "# ============================================================================\n",
    "class VocabWrapper:\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "        self.default_index = None\n",
    "    \n",
    "    def __call__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.stoi[token]\n",
    "        elif self.default_index is not None:\n",
    "            return self.default_index\n",
    "        else:\n",
    "            raise KeyError(f\"Token '{token}' not in vocabulary\")\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        if token in self.stoi:\n",
    "            return self.stoi[token]\n",
    "        elif self.default_index is not None:\n",
    "            return self.default_index\n",
    "        else:\n",
    "            raise KeyError(f\"Token '{token}' not in vocabulary\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def insert_token(self, token, idx):\n",
    "        self.stoi[token] = idx\n",
    "        self.itos[idx] = token\n",
    "    \n",
    "    def set_default_index(self, idx):\n",
    "        self.default_index = idx\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET WRAPPER\n",
    "# ============================================================================\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.dataset = hf_dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        return (item['label'], item['text'])\n",
    "\n",
    "# ============================================================================\n",
    "# COLLATE FUNCTION\n",
    "# ============================================================================\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        encoded = []\n",
    "        for token in tokenizer(_text):\n",
    "            idx = vocab[token]\n",
    "            encoded.append(idx)\n",
    "        \n",
    "        processed_text = torch.tensor(encoded, dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(processed_text.size(0))\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    padded_text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
    "    return padded_text_list, label_list, torch.tensor(lengths)\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset_hf = load_dataset('stanfordnlp/imdb', split='train')\n",
    "test_dataset_hf = load_dataset('stanfordnlp/imdb', split='test')\n",
    "\n",
    "# Split train into train and valid (80/20 split)\n",
    "split_datasets = train_dataset_hf.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset_hf = split_datasets['train']\n",
    "valid_dataset_hf = split_datasets['test']\n",
    "\n",
    "# ============================================================================\n",
    "# BUILD VOCABULARY\n",
    "# ============================================================================\n",
    "print(\"Building vocabulary...\")\n",
    "token_counts = Counter()\n",
    "for example in train_dataset_hf:\n",
    "    tokens = tokenizer(example['text'])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create vocab with special tokens FIRST\n",
    "vocab = VocabWrapper()\n",
    "vocab.insert_token(\"<pad>\", 0)\n",
    "vocab.insert_token(\"<unk>\", 1)\n",
    "\n",
    "# Then add all other tokens starting from index 2\n",
    "for idx, (token, count) in enumerate(sorted_by_freq_tuples, start=2):\n",
    "    vocab.insert_token(token, idx)\n",
    "\n",
    "vocab.set_default_index(1)\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINES\n",
    "# ============================================================================\n",
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: 1. if x == 1 else 0.\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATASETS\n",
    "# ============================================================================\n",
    "train_dataset = IMDBDataset(train_dataset_hf)\n",
    "valid_dataset = IMDBDataset(valid_dataset_hf)\n",
    "test_dataset = IMDBDataset(test_dataset_hf)\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                      shuffle=True, collate_fn=collate_batch, num_workers=0)\n",
    "valid_dl = DataLoader(valid_dataset, batch_size=batch_size, \n",
    "                      shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                     shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Valid samples: {len(valid_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Test a batch\n",
    "text_batch, label_batch, length_batch = next(iter(train_dl))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"Text batch: {text_batch.shape}\")\n",
    "print(f\"Label batch: {label_batch.shape}\")\n",
    "print(f\"Lengths: {length_batch[:5]}\")\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL\n",
    "# ============================================================================\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
    "                 n_layers, bidirectional=False, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, dropout=dropout if n_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text, lengths):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        # Pack padded sequence (lengths must be on CPU)\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        \n",
    "        # Pass through RNN\n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Use the last hidden state\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        \n",
    "        # Pass through FC layer\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE MODEL\n",
    "# ============================================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "model = SentimentRNN(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=256,\n",
    "    output_dim=1,\n",
    "    n_layers=2,\n",
    "    bidirectional=True,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING SETUP\n",
    "# ============================================================================\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING FUNCTIONS\n",
    "# ============================================================================\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    count = 0\n",
    "    \n",
    "    for text_batch, label_batch, lengths in dataloader:\n",
    "        text_batch = text_batch.to(device)\n",
    "        label_batch = label_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(text_batch, lengths)\n",
    "        pred = pred.squeeze()\n",
    "        \n",
    "        loss = loss_fn(pred, label_batch)\n",
    "        acc = binary_accuracy(pred, label_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc.item()\n",
    "        count += 1\n",
    "    \n",
    "    return total_acc / count, total_loss / count\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch, lengths in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            label_batch = label_batch.to(device)\n",
    "            \n",
    "            pred = model(text_batch, lengths)\n",
    "            pred = pred.squeeze()\n",
    "            \n",
    "            loss = loss_fn(pred, label_batch)\n",
    "            acc = binary_accuracy(pred, label_batch)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc.item()\n",
    "            count += 1\n",
    "    \n",
    "    return total_acc / count, total_loss / count\n",
    "\n",
    "# ============================================================================\n",
    "# TRAINING LOOP\n",
    "# ============================================================================\n",
    "num_epochs = 5\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    acc_train, loss_train = train(train_dl)\n",
    "    acc_valid, loss_valid = evaluate(valid_dl)\n",
    "    print(f'Epoch {epoch+1} | Train Acc: {acc_train:.4f} | Train Loss: {loss_train:.4f} | Val Acc: {acc_valid:.4f} | Val Loss: {loss_valid:.4f}')\n",
    "\n",
    "# ============================================================================\n",
    "# TEST EVALUATION\n",
    "# ============================================================================\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "acc_test, loss_test = evaluate(test_dl)\n",
    "print(f'Test Accuracy: {acc_test:.4f} | Test Loss: {loss_test:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
